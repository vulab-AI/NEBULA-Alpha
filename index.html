<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="NEBULA: A Unified Ecosystem for Vision-Language-Action Agent Development & Evaluation">
    <meta name="keywords" content="Embodied AI, Vision-Language-Action, Agent Evaluation, Robot-Centric">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>NEBULA: A Unified Ecosystem for Vision-Language-Action Agent Development & Evaluation</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="icon" href="./static/images/nebula_logo.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><img src="./static/images/nebula_logo.png" width="42" style="margin-right: 10px;">NEBULA: A Unified Ecosystem for Emobodied AI Agent Development & Evaluation</h1>            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/JerryPeng0201" target="_blank">Jierui Peng</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/YanyanZhang3106" target="_blank">Yanyan Zhang</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/YichengDuan" target="_blank">Yicheng Duan</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://tuo-liang.github.io/" target="_blank">Tuo Liang</a>,</span>
              <span class="author-block">
                <a href="https://case.edu/engineering/about/faculty-and-staff-directory/vipin-chaudhary" target="_blank">Vipin Chaudhary</a>,</span>
              <span class="author-block">
                <a href="https://yin-yu.github.io/" target="_blank">Yu Yin</a><sup>â€ </sup>
              </span>
              </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Computer & Data Science Department</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Case Western Reserve University</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution&nbsp;&nbsp;&nbsp;<sup>â€ </sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2503.15835" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2503.15835" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/luyr/BARD-GS" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- HuggingFace link -->
                <span class="link-block">
                  <a href="https://huggingface.co/jerrypeng999/CLAIRE" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-cube"></i>
                  </span>
                  <span>HuggingFace</span>
                </a>
              </span>

                <!-- Leaderboard link -->
                <span class="link-block">
                  <a href="https://huggingface.co/jerrypeng999/CLAIRE" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-trophy"></i>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="item" style="text-align: center;">
        <!-- Your image here -->
        <img src="static/images/overall_update.png" alt="intro_example" />
      </div>
      <h2 class="subtitle has-text-centered">
            <b><span style="color: red;">NEBULA</span> unifies fragmented VLA datasets and APIs for cross-dataset training and benchmarking. It introduces a <span style="color: blue;">dual-axis evaluation</span> (capability and stress testing) with <span style="color: blue;">controlled variable isolation</span> for skill-specific diagnosis. With <span style="color: blue;">hierarchical task difficulty</span>, multi-modal annotations, and visual performance summaries, NEBULA converts success rate into a diagnostic signal, exposing failure modes and reliability limits.</b>
      </h2> 
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Abstract -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Abstract</h2>
      </div>
      <div class="content has-text-justified">
        <p>
          The evaluation of Vision-Language-Action (VLA) agents is hindered by the coarse, end-task success metric that fails to provide precise skill diagnosis or measure robustness to real-world perturbations. 
          This challenge is exacerbated by a fragmented data landscape that impedes reproducible research and the development of generalist models. 
          To address these limitations, we introduce <b>NEBULA</b>, a unified ecosystem for single-arm manipulation that enables diagnostic and reproducible evaluation. NEBULA features a novel dual-axis evaluation protocol that combines fine-grained <i>capability tests</i> for precise skill diagnosis with systematic <i>stress tests</i> that measure robustness.
          A standardized API and a large-scale, aggregated dataset are provided to reduce fragmentation and support cross-dataset training and fair comparison.
          Using NEBULA, we demonstrate that top-performing VLAs struggle with key capabilities such as spatial reasoning and dynamic adaptation, which are consistently obscured by conventional end-task success metrics. By measuring both what an agent can do and when it does so reliably, NEBULA provides a practical foundation for robust, general-purpose embodied agents.
        </p>
    </div>
  </div>
</section>
<!-- End Abstract -->

<!-- Task Summary -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Dual-Axis Evaluation Framework</h2>
      </div>
      <div class="columns is-centered has-text-centered">
        <h3 class="subtitle is-4"><b>Capability Test</b></h3>
      </div>
      <div class="item">
        <img src="static/images/capability_tasks.png" alt="Capability Tasks"/>
      </div>
      <div class="content has-text-justified">
        <p>
          <b>Examples of NEBULA Capability Test task</b> across six core capabilities (Control, Perception, Dynamic Adaptation, Language, Spatial Reasoning, and Robustness) organized into three difficulty levels. Tasks isolate specific skills with controlled complexity. <span style="color: green;">Green</span> marks objects, <span style="color: red;">red</span> marks targets, and <span style="color: blue;">blue</span> indicates contextual cues. <b><u>Bold underlined</u></b> text shows actions; <i><u>italic underlined</u></i> text gives clarifications.
        </p>
        <p>
          The primary impact of this evaluation design is its ability to provide a deep, diagnostic analysis of an agent's abilities, moving beyond ambiguous, task-level success rates. By using a principle of controlled-variable isolation, each task family is designed to test a single capability while minimizing the influence of others, ensuring that performance can be unambiguously attributed to the specific skill being evaluated. This methodology is used to pinpoint the precise reasons for an agent's failure, transforming a simple success metric into an interpretable, multi-faceted signal that reveals specific weaknesses, such as difficulties with spatial reasoning or dynamic adaptation, which are often obscured in traditional benchmarks.
        </p>
      </div>
      
      <div class="columns is-centered has-text-centered">
        <h3 class="subtitle is-4"><b>Stress Test</b></h3>
      </div>
      <div class="item" style="text-align: center;">
        <img src="static/images/stress_test.png" alt="Stress Test" style="width: 70%; height: auto;"/>
      </div>
      <div class="content has-text-justified">
        <p>
          This figure provides examples from the <b>NEBULA Stress Test</b> suite, illustrating the Stability and Adaptability tasks across three progressive difficulty levels. The Stability Score tasks evaluate the smoothness of an agent's actions by increasing the precision required, moving from a simple stack at Level 1 to a more complex, multi-object arrangement at Level 3. The Adaptability tasks assess an agent's capacity to adjust to dynamic changes during execution, beginning with a sudden object movement (Level 1), advancing to a mid-task instruction change (Level 2), and culminating in a command abort that requires rapid re-planning (Level 3).
        </p>
        <p>
          The impact of these Stress Tests is to enable a more profound and diagnostic evaluation than what is possible with traditional success rates, by quantifying system performance under targeted operational pressures. This approach can be used to identify critical performance bottlenecks, reveal an agent's failure points under pressure, and assess its readiness for real-world deployment where conditions are often unpredictable. To specifically diagnose issues in dynamic scenarios, metrics like inference speed and latency are evaluated in conjunction with the Adaptability test to understand how an agent's system responsiveness directly impacts its ability to handle real-time environmental changes and goal shifts.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Task Summary -->

<!-- NEBULA Dataset -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">NEBULA Dataset</h2>
      </div>
      <div class="item" style="text-align: center;">
        <img src="static/images/data_table.png" alt="NEBULA Dataset" style="width: 50%; height: auto;"/>
      </div>
      <div class="content has-text-justified">
        <p>
          To facilitate reproducible and scalable research, NEBULA provides two large-scale, aggregated dataset variants designed to balance completeness with ease of use. All data is collected within NEBULA Ecosystem, ensuring consistency across all tasks.
        </p>
        <ul>
          <li><b>NEBULA-Alpha:</b> The full-scale dataset containing over 222,000 expert demonstrations across the five core capability families. This version is generated entirely using expert trajectories from motion planning and is ideal for training robust, large-scale models.</li>
          <li><b>NEBULA-Beta:</b> A compact, lightweight version containing 10% of the data per task, designed for rapid development, prototyping, and ablation studies. For high-difficulty tasks, this version includes data from human teleoperation to capture more diverse and realistic behaviors.</li>
        </ul>
        <p>
          Both datasets provide multimodal inputs, including videos, language instructions, and trajectories, and are available in standardized PyTorch and TFRecord formats to reduce integration overhead.
        </p>
        <p>
          To access the NEBULA dataset, please refer to our HuggingFace repository.
        </p>
        <p>
          NEBULA also provides the tools necessary for you to generate your own customized datasets. We support two primary methods for data collection: automated Motion Planning and manual Human Teleoperation.
        </p>
      </div>
      <div class="item" style="text-align: center;">
        <img src="static/images/MP_banner.png" alt="Motion Planning" style="width: 100%; height: auto;"/>
      </div>
      <div class="content has-text-justified">
        <p>
          <b>Motion Planning:</b> For generating large-scale expert trajectories automatically, you can use our motion planning pipeline. This process utilizes robot descriptions and motion planners within our simulation environments to generate optimal solutions and collect data systematically.
        </p>
      </div>
      <div class="item" style="text-align: center;">
        <img src="static/images/teleop_banner.png" alt="Teleoperation" style="width: 100%; height: auto;"/>
      </div>
      <div class="content has-text-justified">
        <p>
          <b>Human Teleoperation:</b> To capture more diverse and realistic behaviors for complex tasks, we provide an intuitive teleoperation interface. This system allows you to manually control the robot and record demonstrations. As shown, it is designed for cross-platform use, supporting both macOS and Ubuntu.
        </p>
        <p>
          For more details, please refer to our GitHub repository.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End NEBULA Dataset -->

<!-- NEBULA Unified Data Platform -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">NEBULA Unified Data Platform</h2>
      </div>
      <div class="item" style="text-align: center;">
        <img src="static/images/data.png" alt="NEBULA Unified Data Platform" style="width: 70%; height: auto;"/>
      </div>
      <div class="content has-text-justified">
        <p>
          NEBULA introduces a unified data platform designed to streamline research and foster collaboration. Our platform ingests these varied data formats into a single, standardized NEBULA format, providing the core infrastructure for both unified training and reproducible evaluation. This allows researchers to focus on innovation rather than data engineering.
        </p>
        <ul>
          <li><b>Unified & Structured Data Format:</b> Introducing a standardized data schema for consistent representation of robot interactions in an episode structure. Consolidating observations, actions, and metadata for plug-and-play compatibility across datasets.</li>
          <li><b>Extensible, Robot-Agnostic Architecture:</b> Our platform is not hardcoded to specific hardware. Instead, robot properties are defined in a centralized configuration system, allowing support for various arm setups, gripper types, and sensor configurations.</li>
          <li><b>Powerful and Intuitive Python SDK:</b> Includes a high-level Python API that abstracts low-level data loading and indexing. This enables researchers to easily query, filter, sample data, and perform machine learning tasks like train-test splits with minimal code.</li>
          <li><b>Seamless Model Integration:</b> Provides model input adapters for widely used VLA architectures. These adapters bridge the gap between the unified data format and a model's expected input, allowing for immediate benchmarking with minimal integration overhead.</li>
        </ul>
      </div>
    </div>
  </div>
</section>
<!-- End NEBULA Unified Data Platform -->

<!-- NEBULA Simulation -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">NEBULA Simulation</h2>
      </div>
      <div class="item" style="text-align: center;">
        <video autoplay muted loop playsinline controls preload="metadata" style="width: 70%; height: auto;">
          <source src="static/videos/simulation.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="content has-text-justified">
        <p>
          This video showcases a variety of NEBULA tasks simulated across multiple skill dimensions, illustrating our platform's diverse manipulation scenarios. Each clip demonstrates interactions rendered from six distinct camera viewpoints and three sensory modalities (RGB, depth, and segmentation) enabling rich, multi-perspective observation of the agent's behavior. The tasks span key capability families including perception, control, spatial reasoning, and dynamic adaptation, highlighting NEBULA's support for structured, multi-view, and multi-modal benchmarking in embodied AI research.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End NEBULA Simulation -->

<!-- What does NEBULA reveal -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Current Bottlenecks</h2>
      </div>
      <div class="item" style="text-align: center;">
        <img src="static/images/overview_capability.png" alt="NEBULA capability analysis"/>
      </div>
      <div class="content has-text-justified">
        <p>
          This figure presents two radar charts summarizing model performance across six capability task families. The left chart shows the mean and standard deviation of success rates across all models for each task family, broken down by three difficulty levels. The right chart displays the average performance of individual models on Easy and Medium tasks, which allows for a direct comparison across different architectures.
        </p>
        <p>
          The charts reveal that while most models demonstrate strong performance in Perception and Language tasks, their capabilities in other areas are much more varied. Spatial reasoning remains a key bottleneck for the majority of models. Furthermore, all models struggle significantly with Dynamic Adaptation and Robustness tasks, with performance approaching zero at higher difficulty levels. These results expose a critical gap in current VLA capabilities and highlight an urgent need for research into real-time adaptive planning and out-of-distribution generalization.
        </p>
      </div>
      <div class="item" style="text-align: center;">
        <img src="static/images/stress_result.png" alt="NEBULA stress test results"/>
      </div>
      <div class="content has-text-justified">
        <p>
          This figure shows the stress test evaluations for four different models, comparing their performance across three increasing stress levels. The evaluation consists of four distinct tests: inference frequency, measured in Hertz; latency, measured in milliseconds; a stability score, on a scale from 0 to 1; and adaptability, measured by success rate.
        </p>
        <p>
          These results reveal a consistent degradation in performance for all evaluated models as stress levels increase, highlighting their sensitivity to deployment challenges. GR00T is the most resilient model, maintaining a high inference frequency and demonstrating a modest ability to adapt to changing conditions. In contrast, most other models fail almost completely in the adaptability test and show significant weaknesses in either speed or stability, indicating that they are not yet equipped to handle the demands of dynamic, real-world environments.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End What does NEBULA reveal -->

<!-- Acknowledgements -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Acknowledgements</h2>
      </div>
      <div class="content has-text-justified">
        <p>
          NEBULA is built on top of the excellent work from <b>Sapien</b> and <a href="https://github.com/haosulab/ManiSkill/tree/main" target="_blank"><b>ManiSkill3</b></a>. We have leveraged their logic and assets throughout the development of our platform. We express our deep gratitude and sincere respect to their development teams for making such powerful tools openly available to the community.
        </p>
        <p style="text-align: center; font-size: 1.5rem;">
          Salute! ðŸ«¡
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Acknowledgements -->


<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">Citation</h2>
        <p>If you find our work helpful, please consider cite us:</p>
        <pre><code>@article{lu2025bard,
  title={BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting},
  author={Lu, Yiren and Zhou, Yunlai and Liu, Disheng and Liang, Tuo and Yin, Yu},
  journal={arXiv preprint arXiv:2503.15835},
  year={2025}
}
</code></pre>
    </div>
</section>


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>Website template borrowed from <a href="https://vulab-ai.github.io/YESBUT_Homepage/">YesBut Benchmark</a>.</p>
                    <p>
                        This website is licensed under a <a rel="license"
                                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
