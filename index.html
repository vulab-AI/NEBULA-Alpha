<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="NEBULA: A Unified Ecosystem for Vision-Language-Action Agent Development & Evaluation">
    <meta name="keywords" content="Embodied AI, Vision-Language-Action, Agent Evaluation, Robot-Centric">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>NEBULA: A Unified Ecosystem for Vision-Language-Action Agent Development & Evaluation</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="icon" href="./static/images/nebula_logo.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><img src="./static/images/nebula_logo.png" width="42" style="margin-right: 10px;">NEBULA: A Unified Ecosystem for Emobodied AI Agent Development & Evaluation</h1>            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/JerryPeng0201" target="_blank">Jierui Peng</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/YanyanZhang3106" target="_blank">Yanyan Zhang</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/YichengDuan" target="_blank">Yicheng Duan</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://tuo-liang.github.io/" target="_blank">Tuo Liang</a>,</span>
              <span class="author-block">
                <a href="https://case.edu/engineering/about/faculty-and-staff-directory/vipin-chaudhary" target="_blank">Vipin Chaudhary</a>,</span>
              <span class="author-block">
                <a href="https://yin-yu.github.io/" target="_blank">Yu Yin</a><sup>â€ </sup>
              </span>
              </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Computer & Data Science Department</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Case Western Reserve University</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution&nbsp;&nbsp;&nbsp;<sup>â€ </sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2503.15835" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2503.15835" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/luyr/BARD-GS" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- HuggingFace link -->
                <span class="link-block">
                  <a href="https://huggingface.co/jerrypeng999/CLAIRE" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-cube"></i>
                  </span>
                  <span>HuggingFace</span>
                </a>
              </span>

                <!-- Leaderboard link -->
                <span class="link-block">
                  <a href="https://huggingface.co/jerrypeng999/CLAIRE" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-trophy"></i>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="item" style="text-align: center;">
        <!-- Your image here -->
        <img src="static/images/overall1.png" alt="intro_example" />
      </div>
      <h2 class="subtitle has-text-centered">
            <b><span style="color: red;">NEBULA</span> unifies fragmented VLA datasets and APIs for cross-dataset training and benchmarking. It introduces a <span style="color: blue;">dual-axis evaluation</span> (capability and stress testing) with <span style="color: blue;">controlled variable isolation</span> for skill-specific diagnosis. With <span style="color: blue;">hierarchical task difficulty</span>, multi-modal annotations, and visual performance summaries, NEBULA converts success rate into a diagnostic signal, exposing failure modes and reliability limits.</b>
      </h2> 
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Abstract -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Abstract</h2>
      </div>
      <div class="content has-text-justified">
        <p>
          The evaluation of Vision-Language-Action (VLA) agents is hindered by the coarse, end-task success metric that fails to provide precise skill diagnosis or measure robustness to real-world perturbations. 
          This challenge is exacerbated by a fragmented data landscape that impedes reproducible research and the development of generalist models. 
          To address these limitations, we introduce <b>NEBULA</b>, a unified ecosystem for single-arm manipulation that enables diagnostic and reproducible evaluation. NEBULA features a novel dual-axis evaluation protocol that combines fine-grained <i>capability tests</i> for precise skill diagnosis with systematic <i>stress tests</i> that measure robustness.
          A standardized API and a large-scale, aggregated dataset are provided to reduce fragmentation and support cross-dataset training and fair comparison.
          Using NEBULA, we demonstrate that top-performing VLAs struggle with key capabilities such as spatial reasoning and dynamic adaptation, which are consistently obscured by conventional end-task success metrics. By measuring both what an agent can do and when it does so reliably, NEBULA provides a practical foundation for robust, general-purpose embodied agents.
        </p>
    </div>
  </div>
</section>
<!-- End Abstract -->

<!-- Task Summary -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Evaluation Tasks</h2>
      </div>
      <div class="item">
        <img src="static/images/tasks.png" alt="MY ALT TEXT"/>
      </div>
      <div class="content has-text-justified">
        <p>
          <b>Examples of NEBULA Capability Test task</b> across six core capabilities (Control, Perception, Dynamic Adaptation, Language, Spatial Reasoning, and Robustness) organized into three difficulty levels. Tasks isolate specific skills with controlled complexity. <span style="color: green;">Green</span> marks objects, <span style="color: red;">red</span> marks targets, and <span style="color: blue;">blue</span> indicates contextual cues. <b><u>Bold underlined</u></b> text shows actions; <i><u>italic underlined</u></i> text gives clarifications.
        </p>
    </div>
  </div>
</section>
<!-- End Task Summary -->

<!-- NEBULA Simulation -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">NEBULA Simulation</h2>
      </div>
      <div class="item" style="text-align: center;">
        <video autoplay muted loop playsinline controls preload="metadata" style="width: 70%; height: auto;">
          <source src="static/videos/simulations.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="content has-text-justified">
        <p>
          This video showcases a variety of NEBULA tasks simulated across multiple skill dimensions, illustrating our platform's diverse manipulation scenarios. Each clip demonstrates interactions rendered from six distinct camera viewpoints and three sensory modalities (RGB, depth, and segmentation) enabling rich, multi-perspective observation of the agent's behavior. The tasks span key capability families including perception, control, spatial reasoning, and dynamic adaptation, highlighting NEBULA's support for structured, multi-view, and multi-modal benchmarking in embodied AI research.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End NEBULA Simulation -->

<!-- What does NEBULA reveal -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">What does NEBULA reveal?</h2>
      </div>
      <div class="item" style="text-align: center;">
        <img src="static/images/overview_capability.png" alt="NEBULA capability analysis"/>
      </div>
      <div class="content has-text-justified">
        <p>
          This figure presents two radar charts summarizing model performance across six capability task families. The left chart shows the mean and standard deviation of success rates across all models for each task family, broken down by three difficulty levels. The right chart displays the average performance of individual models on Easy and Medium tasks, which allows for a direct comparison across different architectures.
        </p>
        <p>
          The charts reveal that while most models demonstrate strong performance in Perception and Language tasks, their capabilities in other areas are much more varied. Spatial reasoning remains a key bottleneck for the majority of models. Furthermore, all models struggle significantly with Dynamic Adaptation and Robustness tasks, with performance approaching zero at higher difficulty levels. These results expose a critical gap in current VLA capabilities and highlight an urgent need for research into real-time adaptive planning and out-of-distribution generalization.
        </p>
      </div>
      <div class="item" style="text-align: center;">
        <img src="static/images/stress_result.png" alt="NEBULA stress test results"/>
      </div>
      <div class="content has-text-justified">
        <p>
          This figure shows the stress test evaluations for four different models, comparing their performance across three increasing stress levels. The evaluation consists of four distinct tests: inference frequency, measured in Hertz; latency, measured in milliseconds; a stability score, on a scale from 0 to 1; and adaptability, measured by success rate.
        </p>
        <p>
          These results reveal a consistent degradation in performance for all evaluated models as stress levels increase, highlighting their sensitivity to deployment challenges. GR00T is the most resilient model, maintaining a high inference frequency and demonstrating a modest ability to adapt to changing conditions. In contrast, most other models fail almost completely in the adaptability test and show significant weaknesses in either speed or stability, indicating that they are not yet equipped to handle the demands of dynamic, real-world environments.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End What does NEBULA reveal -->

<!-- Acknowledgements -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Acknowledgements</h2>
      </div>
      <div class="content has-text-justified">
        <p>
          NEBULA is built on top of the excellent work from <b>Sapien</b> and <a href="https://github.com/haosulab/ManiSkill/tree/main" target="_blank"><b>ManiSkill3</b></a>. We have leveraged their logic and assets throughout the development of our platform. We express our deep gratitude and sincere respect to their development teams for making such powerful tools openly available to the community.
        </p>
        <p style="text-align: center; font-size: 1.5rem;">
          Salute! ðŸ«¡
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Acknowledgements -->


<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">Citation</h2>
        <p>If you find our work helpful, please consider cite us:</p>
        <pre><code>@article{lu2025bard,
  title={BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting},
  author={Lu, Yiren and Zhou, Yunlai and Liu, Disheng and Liang, Tuo and Yin, Yu},
  journal={arXiv preprint arXiv:2503.15835},
  year={2025}
}
</code></pre>
    </div>
</section>


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>Website template borrowed from <a href="https://vulab-ai.github.io/YESBUT_Homepage/">YesBut Benchmark</a>.</p>
                    <p>
                        This website is licensed under a <a rel="license"
                                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
